{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "piCXukoLj9QF"
      },
      "outputs": [],
      "source": [
        "# Install LlamaIndex core + HuggingFace embeddings\n",
        "\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install transformers accelerate bitsandbytes\n",
        "\n",
        "\n",
        "#!pip install llama-index sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHEL3YlPkULu"
      },
      "outputs": [],
      "source": [
        "\n",
        "from llama_index.core import VectorStoreIndex, Document, Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCGAQ6SplTIy"
      },
      "source": [
        "I THINK I MIGHT NEED TO DELETE IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jtra3FQzlJNH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from transformers import BitsAndBytesConfig\n",
        "print(userdata.get('HF_TOKEN'))\n",
        "\n",
        "# Use a model\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# Define quantization config\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16, # Assuming you have a GPU\n",
        ")\n",
        "\n",
        "hf_llm = HuggingFaceLLM(model_name= model_name,\n",
        "                        tokenizer_name= model_name,\n",
        "                        device_map=\"auto\",\n",
        "                        #model_kwargs={\"load_in_4bit\": True} # Deprecated\n",
        "                        model_kwargs={\"quantization_config\": nf4_config}\n",
        "                        )\n",
        "\n",
        "# Example small dataset\n",
        "documents = [Document(text=\"This is a small medical dataset.\")]\n",
        "\n",
        "# Use a local HF embedding model (no OpenAI key needed)\n",
        "#embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "hf_embed = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "Settings.embed_model = hf_embed\n",
        "#Settings.embed_model = embed_model\n",
        "Settings.llm = hf_llm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example small dataset\n",
        "documents = [Document(text=\"This is a small medical dataset.\")]\n",
        "\n",
        "\n",
        "# Build the vector index\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Query the index\n",
        "query_engine = index.as_query_engine(similarity_top_k=2)\n",
        "response = query_engine.query(\"What is this dataset about?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gmwETJgluqE"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index-vector-stores-faiss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWtWS356luO4"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores.faiss import FaissVectorStore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TRmtaFsosy5w"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-gpu tqdm llama-index-vector-stores-faiss\n",
        "\n",
        "import os, glob, torch, faiss\n",
        "from math import ceil\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from llama_index.core import Document, VectorStoreIndex, StorageContext, Settings\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "\n",
        "# ✅ Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Paths\n",
        "folder_path = \"/content/drive/My Drive/datasets/med-dialog/large-english-dialog-corpus/*.txt\"\n",
        "faiss_path = \"/content/drive/My Drive/datasets/med-dialog/faiss_index\"\n",
        "\n",
        "# ✅ Load documents\n",
        "documents = []\n",
        "for file in glob.glob(folder_path):\n",
        "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "        documents.append(Document(text=f.read().strip()))\n",
        "\n",
        "print(f\"📂 Total documents: {len(documents)}\")\n",
        "\n",
        "# ✅ Detect GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"✅ Using device: {device}\")\n",
        "\n",
        "# ✅ Hugging Face embeddings (GPU if available)\n",
        "hf_embed = HuggingFaceEmbedding(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    device=device\n",
        ")\n",
        "Settings.embed_model = hf_embed\n",
        "Settings.llm = None  # Disable LLM for indexing\n",
        "\n",
        "# ✅ Chunk large documents\n",
        "parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=50)\n",
        "all_nodes = parser.get_nodes_from_documents(documents)\n",
        "print(f\"✅ After chunking: {len(all_nodes)} chunks generated from {len(documents)} docs\")\n",
        "\n",
        "# ✅ FAISS initialization\n",
        "embedding_dim = hf_embed._model.get_sentence_embedding_dimension()\n",
        "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# ✅ Batch processing with progress bar\n",
        "batch_size = 64  # larger batch size since we're chunking\n",
        "num_nodes = len(all_nodes)\n",
        "num_batches = ceil(num_nodes / batch_size)\n",
        "print(f\"🔄 Starting indexing in {num_batches} batches (batch size = {batch_size})...\")\n",
        "\n",
        "for i in tqdm(range(num_batches), desc=\"📌 Indexing batches\"):\n",
        "    start = i * batch_size\n",
        "    end = min(start + batch_size, num_nodes)\n",
        "    batch_nodes = all_nodes[start:end]\n",
        "\n",
        "    VectorStoreIndex.from_documents(batch_nodes, embed_model=hf_embed, storage_context=storage_context)\n",
        "\n",
        "    # ✅ Save progress\n",
        "    storage_context.persist(persist_dir=faiss_path)\n",
        "\n",
        "print(\"🎉 All chunks indexed and saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR9rDtz4lebM"
      },
      "outputs": [],
      "source": [
        "import os, glob, torch\n",
        "from math import ceil\n",
        "from google.colab import drive, userdata\n",
        "from llama_index.core import Document, VectorStoreIndex, StorageContext, Settings\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from transformers import BitsAndBytesConfig\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "# ✅ Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Paths\n",
        "folder_path = \"/content/drive/My Drive/datasets/med-dialog/large-english-dialog-corpus/*.txt\"\n",
        "faiss_path = \"/content/drive/My Drive/datasets/med-dialog/faiss_index\"\n",
        "\n",
        "# ✅ Load all documents\n",
        "documents = []\n",
        "for file in glob.glob(folder_path):\n",
        "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "        documents.append(Document(text=f.read().strip()))\n",
        "\n",
        "print(f\"📂 Total documents: {len(documents)}\")\n",
        "\n",
        "# ✅ Hugging Face embeddings\n",
        "hf_embed = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "\n",
        "                                )\n",
        "Settings.embed_model = hf_embed\n",
        "Settings.llm = None  # Disable LLM during indexing for speed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC9fJtyfnzms"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwPoXFgOmimg"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "# Get embedding dimension\n",
        "embedding_dim = hf_embed._model.get_sentence_embedding_dimension()\n",
        "\n",
        "# ✅ Create FAISS L2 index\n",
        "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "# Create a new FAISS store\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tXA4KkVouPK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PWEp0-6pmj5"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLKpae01pyxV"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-FA91Q0rLzI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1fyaR0orUzF"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "_ = hf_embed.get_text_embedding(documents[0].text)\n",
        "print(\"⏱️ Embedding time for 1 doc:\", time.time()-start, \"seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZRDXUuyot9O"
      },
      "outputs": [],
      "source": [
        "# ✅ Batch processing\n",
        "batch_size = 32\n",
        "num_docs = len(documents)\n",
        "num_batches = ceil(num_docs / batch_size)\n",
        "\n",
        "print(f\"🔄 Starting batch indexing: {num_batches} batches (batch size = {batch_size})\")\n",
        "\n",
        "for i in tqdm(range(num_batches), desc=\"📌 Indexing batches\"):\n",
        "    start = i * batch_size\n",
        "    end = min(start + batch_size, num_docs)\n",
        "    batch_docs = documents[start:end]\n",
        "\n",
        "    print(f\"📌 Batch {i+1}/{num_batches} → Documents {start+1}-{end}\")\n",
        "    VectorStoreIndex.from_documents(batch_docs, embed_model=hf_embed, storage_context=storage_context)\n",
        "\n",
        "    # ✅ Save after each batch\n",
        "    storage_context.persist(persist_dir=faiss_path)\n",
        "    print(f\"✅ Batch {i+1} saved to {faiss_path}\")\n",
        "\n",
        "print(\"🎉 All batches indexed and saved successfully!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKhryIzTotl3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7CZNJ7emXKm"
      },
      "outputs": [],
      "source": [
        "# ✅ Initialize FAISS vector store\n",
        "vector_store = FaissVectorStore(dim=hf_embed._model.get_sentence_embedding_dimension())\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# ✅ Batching setup\n",
        "batch_size = 32\n",
        "num_docs = len(documents)\n",
        "num_batches = ceil(num_docs / batch_size)\n",
        "\n",
        "print(f\"🔄 Processing {num_batches} batches (batch size = {batch_size})...\")\n",
        "\n",
        "# ✅ Process in batches and save after each\n",
        "for i in range(num_batches):\n",
        "    start = i * batch_size\n",
        "    end = min(start + batch_size, num_docs)\n",
        "    batch_docs = documents[start:end]\n",
        "\n",
        "    print(f\"📌 Batch {i+1}/{num_batches} ({start+1}-{end}) indexing...\")\n",
        "    VectorStoreIndex.from_documents(batch_docs, embed_model=hf_embed, storage_context=storage_context)\n",
        "\n",
        "    # ✅ Persist after each batch\n",
        "    storage_context.persist(persist_dir=faiss_path)\n",
        "    print(f\"✅ Batch {i+1}/{num_batches} saved to {faiss_path}\")\n",
        "\n",
        "print(\"🎉 All batches indexed and saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ-wdRc4VAJZ"
      },
      "outputs": [],
      "source": [
        "# ✅ TASK 1: Mount Google Drive\n",
        "print(\"🔄 TASK 1: Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"✅ TASK 1 Completed: Drive mounted.\")\n",
        "\n",
        "# ✅ TASK 2: Load text files\n",
        "print(\"🔄 TASK 2: Loading dataset files...\")\n",
        "folder_path = \"/content/drive/My Drive/datasets/med-dialog/large-english-dialog-corpus/*.txt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x__yHWDeSsqN"
      },
      "outputs": [],
      "source": [
        "import glob, os\n",
        "from llama_index.core import Document\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "documents = []\n",
        "for file in glob.glob(folder_path):\n",
        "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read().strip()\n",
        "        documents.append(Document(text=text))\n",
        "\n",
        "print(f\"✅ TASK 2 Completed: Loaded {len(documents)} documents.\")\n",
        "\n",
        "# Collect all text files\n",
        "files = glob.glob(folder_path)\n",
        "\n",
        "print(f\"📂 Found {len(files)} documents in the folder.\")\n",
        "\n",
        "# Optionally, list first few file names\n",
        "for f in files[:5]:\n",
        "    print(\"➡️\", f)\n",
        "\n",
        "# ✅ TASK 3: Preview\n",
        "if documents:\n",
        "    print(\"🔄 TASK 3: Showing document previews...\")\n",
        "    for i, doc in enumerate(documents[:6]):\n",
        "        print(f\"\\n--- Document {i+1} preview ---\\n\", doc.text[:300])\n",
        "    print(\"✅ TASK 3 Completed: Preview displayed.\")\n",
        "else:\n",
        "    print(\"⚠️ No documents found. Check folder path or file extensions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOem32ZnkWFf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1gNqECukVN1"
      },
      "outputs": [],
      "source": [
        "import os, glob, torch\n",
        "from math import ceil\n",
        "from google.colab import drive, userdata\n",
        "from llama_index.core import Document, VectorStoreIndex, StorageContext, Settings\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from transformers import BitsAndBytesConfig\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "# ✅ Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Paths\n",
        "folder_path = \"/content/drive/My Drive/datasets/med-dialog/large-english-dialog-corpus/*.txt\"\n",
        "faiss_path = \"/content/drive/My Drive/datasets/med-dialog/faiss_index\"\n",
        "\n",
        "# ✅ Load all documents\n",
        "documents = []\n",
        "for file in glob.glob(folder_path):\n",
        "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "        documents.append(Document(text=f.read().strip()))\n",
        "\n",
        "print(f\"📂 Total documents: {len(documents)}\")\n",
        "\n",
        "# ✅ Hugging Face embeddings\n",
        "hf_embed = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "Settings.embed_model = hf_embed\n",
        "Settings.llm = None  # Disable LLM during indexing for speed\n",
        "\n",
        "# ✅ Initialize FAISS vector store\n",
        "vector_store = FaissVectorStore(dim=hf_embed._model.get_sentence_embedding_dimension())\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# ✅ Batching setup\n",
        "batch_size = 32\n",
        "num_docs = len(documents)\n",
        "num_batches = ceil(num_docs / batch_size)\n",
        "\n",
        "print(f\"🔄 Processing {num_batches} batches (batch size = {batch_size})...\")\n",
        "\n",
        "# ✅ Process in batches and save after each\n",
        "for i in range(num_batches):\n",
        "    start = i * batch_size\n",
        "    end = min(start + batch_size, num_docs)\n",
        "    batch_docs = documents[start:end]\n",
        "\n",
        "    print(f\"📌 Batch {i+1}/{num_batches} ({start+1}-{end}) indexing...\")\n",
        "    VectorStoreIndex.from_documents(batch_docs, embed_model=hf_embed, storage_context=storage_context)\n",
        "\n",
        "    # ✅ Persist after each batch\n",
        "    storage_context.persist(persist_dir=faiss_path)\n",
        "    print(f\"✅ Batch {i+1}/{num_batches} saved to {faiss_path}\")\n",
        "\n",
        "print(\"🎉 All batches indexed and saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3DSaTcAkU00"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzJ6oMYIYcEf"
      },
      "outputs": [],
      "source": [
        "import os, glob, torch\n",
        "from google.colab import drive, userdata\n",
        "from llama_index.core import Document, VectorStoreIndex, Settings\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# ✅ TASK 1: Mount Google Drive\n",
        "print(\"🔄 TASK 1: Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"✅ TASK 1 Completed: Drive mounted.\")\n",
        "\n",
        "# ✅ TASK 2: Load dataset from Drive\n",
        "folder_path = \"/content/drive/My Drive/datasets/med-dialog/large-english-dialog-corpus/*.txt\"\n",
        "documents = []\n",
        "for file in glob.glob(folder_path):\n",
        "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read().strip()\n",
        "        documents.append(Document(text=text))\n",
        "\n",
        "print(f\"✅ TASK 2 Completed: Loaded {len(documents)} documents.\")\n",
        "\n",
        "# ✅ Count and preview\n",
        "files = glob.glob(folder_path)\n",
        "print(f\"📂 Found {len(files)} documents in the folder.\")\n",
        "for f in files[:5]:\n",
        "    print(\"➡️\", f)\n",
        "\n",
        "if documents:\n",
        "    print(\"🔄 TASK 3: Showing document previews...\")\n",
        "    for i, doc in enumerate(documents[:3]):\n",
        "        print(f\"\\n--- Document {i+1} preview ---\\n\", doc.text[:300])\n",
        "    print(\"✅ TASK 3 Completed: Preview displayed.\")\n",
        "else:\n",
        "    print(\"⚠️ No documents found. Check folder path or file extensions.\")\n",
        "\n",
        "# ✅ TASK 4: Setup Hugging Face Token\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_token\n",
        "print(\"✅ TASK 4 Completed: Hugging Face token set.\")\n",
        "\n",
        "# ✅ TASK 5: Configure Mistral 7B with 4-bit quantization\n",
        "print(\"🔄 TASK 5: Loading Mistral 7B model...\")\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "hf_llm = HuggingFaceLLM(\n",
        "    model_name=model_name,\n",
        "    tokenizer_name=model_name,\n",
        "    device_map=\"auto\",\n",
        "    model_kwargs={\"quantization_config\": nf4_config}\n",
        ")\n",
        "print(\"✅ TASK 5 Completed: Mistral 7B loaded.\")\n",
        "\n",
        "# ✅ TASK 6: Setup Hugging Face embeddings\n",
        "hf_embed = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "Settings.embed_model = hf_embed\n",
        "Settings.llm = hf_llm\n",
        "print(\"✅ TASK 6 Completed: Embeddings configured.\")\n",
        "\n",
        "# ✅ TASK 7: Build the vector index\n",
        "print(\"🔄 TASK 7: Building vector index...\")\n",
        "index = VectorStoreIndex.from_documents(documents,embed_model=hf_embed, batch_size=32)\n",
        "print(\"✅ TASK 7 Completed: Vector index built.\")\n",
        "\n",
        "# ✅ TASK 8: Test a sample query\n",
        "query_engine = index.as_query_engine(similarity_top_k=2)\n",
        "response = query_engine.query(\"What topics are covered in this medical dataset?\")\n",
        "print(\"🔍 Query Answer:\\n\", response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_YN4rAtj19Y"
      },
      "outputs": [],
      "source": [
        "%who\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOHMwc7YiyDB"
      },
      "outputs": [],
      "source": [
        "index.storage_context.persist(persist_dir=\"/content/drive/My Drive/datasets/med-dialog/faiss_index\")\n",
        "print(\"✅ Index saved to Drive\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}